<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title> - Pytorch</title>
    <link rel="self" type="application/atom+xml" href="https://example.org/tags/pytorch/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://example.org"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2025-01-20T00:00:00+00:00</updated>
    <id>https://example.org/tags/pytorch/atom.xml</id>
    <entry xml:lang="en">
        <title>Fourier Neural Operators</title>
        <published>2025-01-20T00:00:00+00:00</published>
        <updated>2025-01-20T00:00:00+00:00</updated>
        
        <author>
          <name>
            Alyn Musselman
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://example.org/blog/fourier-neural-operators/"/>
        <id>https://example.org/blog/fourier-neural-operators/</id>
        
        <content type="html" xml:base="https://example.org/blog/fourier-neural-operators/">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;&#x2F;h2&gt;
&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;&#x2F;h2&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;&#x2F;h2&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Homeomorphic AutoEncoder</title>
        <published>2024-09-10T00:00:00+00:00</published>
        <updated>2024-09-10T00:00:00+00:00</updated>
        
        <author>
          <name>
            Alyn Musselman
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://example.org/blog/autoencoder-coordinate-mappings/"/>
        <id>https://example.org/blog/autoencoder-coordinate-mappings/</id>
        
        <content type="html" xml:base="https://example.org/blog/autoencoder-coordinate-mappings/">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;&#x2F;h2&gt;
&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;&#x2F;h2&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;&#x2F;h2&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Transformer Bitcoin Trading Bot</title>
        <published>2021-12-15T00:00:00+00:00</published>
        <updated>2021-12-15T00:00:00+00:00</updated>
        
        <author>
          <name>
            Alyn Musselman
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://example.org/blog/bitcoin-trade-bot/"/>
        <id>https://example.org/blog/bitcoin-trade-bot/</id>
        
        <content type="html" xml:base="https://example.org/blog/bitcoin-trade-bot/">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;&#x2F;h2&gt;
&lt;p&gt;I chose to make this project because I needed to get more familiar with ML. I had read all about CNN&#x27;s, Auto Encoders, Variational Auto Encoders, LSTM&#x27;s, RNN&#x27;s and even built a couple of them, but I wanted to make a stand alone ML project. I chose the transformer architecture for 2 reasons. Firstly, it was the most complex architecture out at the time. Secondly, it seemed to handle time-series data fairly well and with lower training cost than some of the other models.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;&#x2F;h2&gt;
&lt;p&gt;To do this project I needed to:
- Build the model from scratch
- Collect + Cleanup old Bitcoin data
- Tokenize the data
- Batchify the training data
- Train the model
- Test the model
- Let it loose&lt;&#x2F;p&gt;
&lt;p&gt;Building the model from scratch took a long time, even with the many tutorials I used:
https:&#x2F;&#x2F;nlp.seas.harvard.edu&#x2F;2018&#x2F;04&#x2F;03&#x2F;attention.html
https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;
https:&#x2F;&#x2F;rscircus.github.io&#x2F;posts&#x2F;2020-02-22-transformer&#x2F;&lt;&#x2F;p&gt;
&lt;p&gt;Luckily the article &quot;The Annotated Transformer&quot; was so detailed that once I had built the model outlined there I only needed to do the data-handling in order to train the model.&lt;&#x2F;p&gt;
&lt;p&gt;The dataset I used was taken from this Kaggle page:
https:&#x2F;&#x2F;www.kaggle.com&#x2F;datasets&#x2F;mczielinski&#x2F;bitcoin-historical-data&lt;&#x2F;p&gt;
&lt;p&gt;There were quite a few NaN and 0 values in the dataset that had to be removed, but lucky for me there ended up being just enough data remaining to train the model.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;&#x2F;h2&gt;
</content>
        
    </entry>
</feed>
